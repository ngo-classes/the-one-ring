services:
  base:
    build:
      context: ./docker/base
    image: linhbngo/onering:base
    command: /bin/bash -c exit
  login:
    build:
      context: ./docker/login
    image: linhbngo/onering:login
    depends_on:
      base:
        condition: service_completed_successfully 
    volumes:
      - "/mnt/mnt/e/workspace/courses:/lectures"
      - "/mnt/mnt/e/workspace/grading-scripts:/grading-scripts"
      - "/mnt/mnt/e/workspace/d2l:/d2l"
      - .:/workspace
      - home:/home
    ports:
      - "18000:8000"
      - "18088:8088"
      - "2222:22"
    mem_limit: 4096m
    cpus: 2.0

  331-head:
    build:
      context: ./docker/csc331
    image: linhbngo/onering:csc331
    depends_on:
      - login
    volumes:
      - "/mnt/e/workspace/courses:/lectures"
      - "/mnt/e/workspace/grading-scripts:/grading-scripts"
      - "/mnt/e/workspace/d2l:/d2l"
      - .:/workspace
      - home:/home
    ports:
      - "22"
    mem_limit: 2048m
    cpus: 2.0

  466-cpn01:
    build:
      context: ./docker/csc466
    image: linhbngo/onering:csc466
    container_name: cpn01
    depends_on:
      - login
    volumes:
      - "/mnt/e/workspace/courses:/lectures"
      - "/mnt/e/workspace/grading-scripts:/grading-scripts"
      - "/mnt/e/workspace/d2l:/d2l"
      - .:/workspace
      - home:/home
    ports:
      - "22"
    mem_limit: 2048m
    cpus: 2.0

  466-cpn02:
    build:
      context: ./docker/csc466
    image: linhbngo/onering:csc466
    container_name: cpn02
    depends_on:
      - login
    volumes:
      - "/mnt/e/workspace/courses:/lectures"
      - "/mnt/e/workspace/grading-scripts:/grading-scripts"
      - "/mnt/e/workspace/d2l:/d2l"
      - .:/workspace
      - home:/home
    ports:
      - "22"
    mem_limit: 2048m
    cpus: 2.0

  ollama:
    container_name: ollama
    image: ollama/ollama
    volumes: # change this to your own external model storage place/platform
    # - //mnt/e/data/ollama_models:/root/.ollama # This is for Windows
      - /Users/lngo/workspace/ollama_models:/root/.ollama # This is for Mac
    ports:
      - "11434:11434"
    mem_limit: 8192m
    environment:
      OLLAMA_RUN_TIMEOUT: 600
      OLLAMA_REQUEST_TIMEOUT: 600
      OLLAMA_LOAD_TIMEOUT: 600

volumes:
  home: